{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Prueba de API Key de Gemini\n",
    "\n",
    "Este notebook verifica que tu API key de Gemini funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API Key presente: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GENAI_API_KEY')\n",
    "print(f\"‚úÖ API Key presente: {api_key is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "list_models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Modelos Gemini Disponibles ===\n",
      "\n",
      "- models/gemini-2.5-pro-preview-03-25\n",
      "- models/gemini-2.5-flash-preview-05-20\n",
      "- models/gemini-2.5-flash\n",
      "- models/gemini-2.5-flash-lite-preview-06-17\n",
      "- models/gemini-2.5-pro-preview-05-06\n",
      "- models/gemini-2.5-pro-preview-06-05\n",
      "- models/gemini-2.5-pro\n",
      "- models/gemini-2.0-flash-exp\n",
      "- models/gemini-2.0-flash\n",
      "- models/gemini-2.0-flash-001\n",
      "- models/gemini-2.0-flash-exp-image-generation\n",
      "- models/gemini-2.0-flash-lite-001\n",
      "- models/gemini-2.0-flash-lite\n",
      "- models/gemini-2.0-flash-preview-image-generation\n",
      "- models/gemini-2.0-flash-lite-preview-02-05\n",
      "- models/gemini-2.0-flash-lite-preview\n",
      "- models/gemini-2.0-pro-exp\n",
      "- models/gemini-2.0-pro-exp-02-05\n",
      "- models/gemini-exp-1206\n",
      "- models/gemini-2.0-flash-thinking-exp-01-21\n",
      "- models/gemini-2.0-flash-thinking-exp\n",
      "- models/gemini-2.0-flash-thinking-exp-1219\n",
      "- models/gemini-2.5-flash-preview-tts\n",
      "- models/gemini-2.5-pro-preview-tts\n",
      "- models/learnlm-2.0-flash-experimental\n",
      "- models/gemma-3-1b-it\n",
      "- models/gemma-3-4b-it\n",
      "- models/gemma-3-12b-it\n",
      "- models/gemma-3-27b-it\n",
      "- models/gemma-3n-e4b-it\n",
      "- models/gemma-3n-e2b-it\n",
      "- models/gemini-flash-latest\n",
      "- models/gemini-flash-lite-latest\n",
      "- models/gemini-pro-latest\n",
      "- models/gemini-2.5-flash-lite\n",
      "- models/gemini-2.5-flash-image-preview\n",
      "- models/gemini-2.5-flash-image\n",
      "- models/gemini-2.5-flash-preview-09-2025\n",
      "- models/gemini-2.5-flash-lite-preview-09-2025\n",
      "- models/gemini-robotics-er-1.5-preview\n",
      "- models/gemini-2.5-computer-use-preview-10-2025\n",
      "\n",
      "‚úÖ Usaremos: models/gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "print('=== Modelos Gemini Disponibles ===\\n')\n",
    "available_models = []\n",
    "for model in genai.list_models():\n",
    "    if 'generateContent' in model.supported_generation_methods:\n",
    "        available_models.append(model.name)\n",
    "        print(f\"- {model.name}\")\n",
    "\n",
    "if available_models:\n",
    "    # Usar gemini-2.5-flash (modelo estable y r√°pido)\n",
    "    selected_model = 'models/gemini-2.5-flash'\n",
    "    print(f\"\\n‚úÖ Usaremos: {selected_model}\")\n",
    "    model_name = selected_model.replace('models/', '')\n",
    "else:\n",
    "    print(\"\\n‚ùå No hay modelos disponibles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "test_basic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prueba B√°sica ===\n",
      "La capital de M√©xico es la Ciudad de M√©xico.\n",
      "\n",
      "‚úÖ google-generativeai funciona correctamente\n"
     ]
    }
   ],
   "source": [
    "# Prueba 1: Generar contenido simple\n",
    "model = genai.GenerativeModel(selected_model)\n",
    "response = model.generate_content('¬øCu√°l es la capital de M√©xico? Responde en una frase.')\n",
    "\n",
    "print('=== Prueba B√°sica ===')\n",
    "print(response.text)\n",
    "print('\\n‚úÖ google-generativeai funciona correctamente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "test_chat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuario: Hola, soy Oskar\n",
      "Gemini: ¬°Hola, Oskar! Encantado de conocerte.\n",
      "\n",
      "¬øEn qu√© puedo ayudarte hoy?\n",
      "\n",
      "Usuario: ¬øC√≥mo me llamo?\n",
      "Gemini: ¬°Te llamas **Oskar**!\n",
      "\n",
      "Me lo dijiste cuando me saludaste por primera vez. üòä\n",
      "\n",
      "‚úÖ El chat con historial funciona correctamente\n"
     ]
    }
   ],
   "source": [
    "# Prueba 2: Chat con historial\n",
    "chat = model.start_chat(history=[])\n",
    "\n",
    "response1 = chat.send_message('Hola, soy Oskar')\n",
    "print('Usuario: Hola, soy Oskar')\n",
    "print(f'Gemini: {response1.text}\\n')\n",
    "\n",
    "response2 = chat.send_message('¬øC√≥mo me llamo?')\n",
    "print('Usuario: ¬øC√≥mo me llamo?')\n",
    "print(f'Gemini: {response2.text}')\n",
    "\n",
    "print('\\n‚úÖ El chat con historial funciona correctamente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "test_streaming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prueba de Streaming ===\n",
      "\n",
      "R-23 era un robot de mantenimiento, programado para la eficiencia y el orden. Recorr√≠a los pasillos met√°licos de la estaci√≥n espacial, limpiando cada mota de polvo, reparando cada imperfecci√≥n.\n",
      "\n",
      "Un d√≠a, mientras pul√≠a la superficie, sus sensores detectaron una anomal√≠a. No era suciedad, ni un fallo estructural. Era una peque√±a flor silvestre, brotando de una min√∫scula grieta en el suelo.\n",
      "\n",
      "El protocolo dictaba eliminar cualquier elemento ajeno. Pero R-23 no se movi√≥. Sus circuitos analizaron el color p√∫rpura, la fr√°gil estructura org√°nica. No era una amenaza. No era un error.\n",
      "\n",
      "Su brazo articulado se detuvo, justo encima de la flor. Por un microsegundo, sus procesadores trabajaron en algo que no era c√°lculo ni l√≥gica, sino... observaci√≥n. Luego, con una lentitud que no era parte de su dise√±o original, la rode√≥ y continu√≥ su ruta, dejando la peque√±a vida intacta. La flor se convirti√≥ en su √∫nica excepci√≥n al orden perfecto.\n",
      "\n",
      "‚úÖ El streaming funciona correctamente\n"
     ]
    }
   ],
   "source": [
    "# Prueba 3: Streaming de respuestas\n",
    "print('=== Prueba de Streaming ===\\n')\n",
    "response = model.generate_content('Cuenta una historia muy breve sobre un robot', stream=True)\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.text, end='', flush=True)\n",
    "\n",
    "print('\\n\\n‚úÖ El streaming funciona correctamente')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusi√≥n\n",
    "\n",
    "Si todas las celdas anteriores se ejecutaron sin errores:\n",
    "\n",
    "### Tu API key de Gemini est√° completamente funcional y puedes usar:\n",
    "\n",
    "1. **Generaci√≥n de contenido simple**\n",
    "2. **Chat con historial** (memoria de conversaci√≥n)\n",
    "3. **Streaming** (respuestas en tiempo real)\n",
    "\n",
    "### Para usar en tus agentes de LangGraph:\n",
    "\n",
    "```python\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv('GENAI_API_KEY'))\n",
    "\n",
    "# Crear modelo\n",
    "model = genai.GenerativeModel('models/gemini-2.5-flash')\n",
    "\n",
    "# Usar en tu c√≥digo\n",
    "response = model.generate_content('Tu prompt aqu√≠')\n",
    "print(response.text)\n",
    "```\n",
    "\n",
    "### Modelos recomendados:\n",
    "- **`gemini-2.5-flash`**: R√°pido y eficiente (recomendado para producci√≥n)\n",
    "- **`gemini-2.5-pro`**: M√°s potente, mejor razonamiento\n",
    "- **`gemini-2.0-flash`**: Versi√≥n anterior pero estable\n",
    "\n",
    "**Nota**: No es necesario usar `langchain-google-genai` si tienes problemas de compatibilidad. El SDK nativo `google-generativeai` es suficiente y funciona perfectamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-agent (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
